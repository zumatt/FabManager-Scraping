{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b700201",
   "metadata": {},
   "source": [
    "# FabManager Data Scraper\n",
    "\n",
    "This notebook provides an interactive interface to scrape different types of data from the FabManager platform using the Open API.\n",
    "\n",
    "## Supported Data Types\n",
    "- **Users**: All user accounts in the system\n",
    "- **Machines**: All available machines/equipment\n",
    "- **Reservations**: All bookings (automatically divided by type: Machine, Training, Event)\n",
    "- **Trainings**: All training sessions\n",
    "\n",
    "## Features\n",
    "- Automatic pagination handling\n",
    "- RFC-5988 compliant pagination\n",
    "- Timestamped output files\n",
    "- Clean JSON export (removes unusual line terminators)\n",
    "- Connection testing before scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa6983",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Generator\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf7581",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your FabManager API credentials here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4013712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FabManager API Configuration\n",
    "BASE_URL = 'Insert_your_FabManager_URL_here'\n",
    "API_TOKEN = 'API_Token_Goes_Here'\n",
    "\n",
    "# Setup export directory\n",
    "EXPORT_DIR = Path.cwd() / 'exports'\n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"✓ Configuration set\")\n",
    "print(f\"  - Base URL: {BASE_URL}\")\n",
    "print(f\"  - Export directory: {EXPORT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307433d9",
   "metadata": {},
   "source": [
    "## 3. Define API Client Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe8c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FabManagerAPIClient:\n",
    "    \"\"\"Client for interacting with the FabManager Open API.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, api_token: str):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.api_token = api_token\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Set default headers\n",
    "        self.session.headers.update({\n",
    "            'Authorization': f'Token token={self.api_token}',\n",
    "            'Accept': 'application/json'\n",
    "        })\n",
    "    \n",
    "    def test_connection(self) -> tuple[bool, str]:\n",
    "        \"\"\"Test the API connection and authentication.\"\"\"\n",
    "        try:\n",
    "            endpoint = f'{self.base_url}/open_api/v1/users'\n",
    "            params = {'page': 1, 'per_page': 1}\n",
    "            \n",
    "            logger.info(\"Testing API connection...\")\n",
    "            response = self.session.get(endpoint, params=params, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return True, \"Connection successful\"\n",
    "            elif response.status_code == 401:\n",
    "                return False, \"Authentication failed - Invalid API token\"\n",
    "            elif response.status_code == 403:\n",
    "                return False, \"Access forbidden - Check API permissions\"\n",
    "            elif response.status_code == 404:\n",
    "                return False, \"API endpoint not found - Check base URL\"\n",
    "            else:\n",
    "                return False, f\"Unexpected response: HTTP {response.status_code}\"\n",
    "                \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            return False, \"Connection error - Check base URL and internet connection\"\n",
    "        except requests.exceptions.Timeout:\n",
    "            return False, \"Connection timeout - Server not responding\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return False, f\"Request error: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Unexpected error: {str(e)}\"\n",
    "    \n",
    "    def _get_endpoint_data(self, endpoint: str, data_key: str, page: int = 1, per_page: int = 100) -> Dict:\n",
    "        \"\"\"Generic method to fetch data from any endpoint.\"\"\"\n",
    "        full_endpoint = f'{self.base_url}{endpoint}'\n",
    "        params = {'page': page, 'per_page': per_page}\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Fetching page {page} from {endpoint}\")\n",
    "            response = self.session.get(full_endpoint, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            pagination_info = self._extract_pagination_info(response.headers)\n",
    "            response_data = response.json()\n",
    "            \n",
    "            if isinstance(response_data, dict) and data_key in response_data:\n",
    "                data = response_data[data_key]\n",
    "            else:\n",
    "                data = response_data\n",
    "            \n",
    "            return {'data': data, 'pagination': pagination_info}\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error fetching data from {endpoint}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _get_all_data(self, endpoint: str, data_key: str, per_page: int = 100, max_pages: Optional[int] = None) -> Generator[Dict, None, None]:\n",
    "        \"\"\"Generator that fetches all data across all pages.\"\"\"\n",
    "        page = 1\n",
    "        \n",
    "        while True:\n",
    "            if max_pages and page > max_pages:\n",
    "                logger.info(f\"Reached max_pages limit ({max_pages})\")\n",
    "                break\n",
    "                \n",
    "            result = self._get_endpoint_data(endpoint, data_key, page, per_page)\n",
    "            data = result['data']\n",
    "            pagination = result['pagination']\n",
    "            \n",
    "            if data:\n",
    "                for item in data:\n",
    "                    yield item\n",
    "            \n",
    "            if not pagination.get('has_next') or not data:\n",
    "                logger.info(f\"Fetched all pages (total: {page})\")\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "    \n",
    "    def fetch_all_as_list(self, endpoint: str, data_key: str, per_page: int = 100, max_pages: Optional[int] = None) -> List[Dict]:\n",
    "        \"\"\"Fetch all data and return as a list.\"\"\"\n",
    "        all_data = list(self._get_all_data(endpoint, data_key, per_page, max_pages))\n",
    "        logger.info(f\"Total items fetched: {len(all_data)}\")\n",
    "        return all_data\n",
    "    \n",
    "    def _extract_pagination_info(self, headers: Dict) -> Dict:\n",
    "        \"\"\"Extract pagination information from response headers.\"\"\"\n",
    "        pagination = {'total': None, 'per_page': None, 'has_next': False, 'has_prev': False, 'links': {}}\n",
    "        \n",
    "        if 'Total' in headers:\n",
    "            try:\n",
    "                pagination['total'] = int(headers['Total'])\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        if 'Per-Page' in headers:\n",
    "            try:\n",
    "                pagination['per_page'] = int(headers['Per-Page'])\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        if 'Link' in headers:\n",
    "            links = self._parse_link_header(headers['Link'])\n",
    "            pagination['links'] = links\n",
    "            pagination['has_next'] = 'next' in links\n",
    "            pagination['has_prev'] = 'prev' in links\n",
    "        \n",
    "        return pagination\n",
    "    \n",
    "    def _parse_link_header(self, link_header: str) -> Dict[str, str]:\n",
    "        \"\"\"Parse RFC-5988 Link header.\"\"\"\n",
    "        links = {}\n",
    "        \n",
    "        for link in link_header.split(','):\n",
    "            link = link.strip()\n",
    "            if not link:\n",
    "                continue\n",
    "            \n",
    "            parts = link.split(';')\n",
    "            if len(parts) >= 2:\n",
    "                url = parts[0].strip().strip('<>')\n",
    "                rel = None\n",
    "                \n",
    "                for part in parts[1:]:\n",
    "                    if 'rel=' in part:\n",
    "                        rel = part.split('=')[1].strip().strip('\"\\'')\n",
    "                        break\n",
    "                \n",
    "                if rel:\n",
    "                    links[rel] = url\n",
    "        \n",
    "        return links\n",
    "\n",
    "print(\"✓ FabManagerAPIClient class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea6cf3e",
   "metadata": {},
   "source": [
    "## 4. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54470a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_for_json(data):\n",
    "    \"\"\"Recursively clean data to remove unusual line terminators.\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        return data.replace('\\u2028', '\\n').replace('\\u2029', '\\n')\n",
    "    elif isinstance(data, dict):\n",
    "        return {key: clean_data_for_json(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [clean_data_for_json(item) for item in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    \"\"\"Return a safe filename string.\"\"\"\n",
    "    sanitized = re.sub(r\"[^A-Za-z0-9_.-]\", \"_\", name)\n",
    "    return sanitized or \"unknown\"\n",
    "\n",
    "def save_data(data: List[Dict], data_key: str, filename_prefix: str, export_dir: Path) -> str:\n",
    "    \"\"\"Save scraped data to a JSON file with timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M\")\n",
    "    filename = f\"FabManager_ExportedData_{filename_prefix}_{timestamp}.json\"\n",
    "    filepath = export_dir / filename\n",
    "    \n",
    "    logger.info(f\"Cleaning data to remove unusual line terminators...\")\n",
    "    cleaned_data = clean_data_for_json(data)\n",
    "    \n",
    "    output_data = {data_key: cleaned_data}\n",
    "    \n",
    "    logger.info(f\"Saving {len(data)} items to {filename}...\")\n",
    "    with open(filepath, 'w', encoding='utf-8', newline='\\n') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return str(filepath)\n",
    "\n",
    "def divide_reservations_by_type(reservations: List[Dict], export_dir: Path) -> Dict[str, str]:\n",
    "    \"\"\"Split reservations into three separate files by reservable_type.\"\"\"\n",
    "    logger.info(\"Dividing reservations by type...\")\n",
    "    \n",
    "    groups = defaultdict(list)\n",
    "    for reservation in reservations:\n",
    "        rtype = reservation.get(\"reservable_type\") or reservation.get(\"type\") or \"unknown\"\n",
    "        if not isinstance(rtype, str):\n",
    "            rtype = str(rtype)\n",
    "        groups[rtype].append(reservation)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%d_%m_%Y_%H-%M\")\n",
    "    saved_files = {}\n",
    "    \n",
    "    for rtype, items in groups.items():\n",
    "        filename = f\"FabManager_ExportedData_Reservations_{sanitize_filename(rtype)}_{timestamp}.json\"\n",
    "        outpath = export_dir / filename\n",
    "        \n",
    "        cleaned_items = clean_data_for_json(items)\n",
    "        \n",
    "        with open(outpath, 'w', encoding='utf-8', newline='\\n') as f:\n",
    "            json.dump({\"reservations\": cleaned_items}, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        saved_files[rtype] = str(outpath)\n",
    "        logger.info(f\"  - {rtype}: {len(items)} items saved to {filename}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92be7d",
   "metadata": {},
   "source": [
    "## 5. Initialize API Client and Test Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API client\n",
    "print(\"Initializing FabManager API client...\")\n",
    "client = FabManagerAPIClient(base_url=BASE_URL, api_token=API_TOKEN)\n",
    "\n",
    "# Test connection\n",
    "print(\"Testing API connection...\")\n",
    "success, message = client.test_connection()\n",
    "\n",
    "if not success:\n",
    "    print(f\"\\n✗ API Connection Failed: {message}\")\n",
    "    print(\"\\nPlease check your configuration in the Configuration cell above.\")\n",
    "else:\n",
    "    print(f\"✓ {message}\")\n",
    "    print(\"✓ API client ready\")\n",
    "    print(\"\\nYou can now proceed to scrape data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ef353",
   "metadata": {},
   "source": [
    "## 6. Scrape Data\n",
    "\n",
    "Run the cells below to scrape different types of data. You can run them individually or all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855878c",
   "metadata": {},
   "source": [
    "### 6.1 Scrape Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Scraping Users\".center(60))\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    users_data = client.fetch_all_as_list(\n",
    "        endpoint='/open_api/v1/users',\n",
    "        data_key='users',\n",
    "        per_page=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Total users fetched: {len(users_data)}\")\n",
    "    \n",
    "    if users_data:\n",
    "        print(f\"\\nSample user data:\")\n",
    "        print(json.dumps(users_data[0], indent=2, ensure_ascii=False)[:500] + \"...\")\n",
    "    \n",
    "    filepath = save_data(users_data, 'users', 'Users', EXPORT_DIR)\n",
    "    print(f\"\\n✓ Successfully saved to: {filepath}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error scraping users: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649fb95",
   "metadata": {},
   "source": [
    "### 6.2 Scrape Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06204d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Scraping Machines\".center(60))\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    machines_data = client.fetch_all_as_list(\n",
    "        endpoint='/open_api/v1/machines',\n",
    "        data_key='machines',\n",
    "        per_page=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Total machines fetched: {len(machines_data)}\")\n",
    "    \n",
    "    if machines_data:\n",
    "        print(f\"\\nSample machine data:\")\n",
    "        print(json.dumps(machines_data[0], indent=2, ensure_ascii=False)[:500] + \"...\")\n",
    "    \n",
    "    filepath = save_data(machines_data, 'machines', 'Machines', EXPORT_DIR)\n",
    "    print(f\"\\n✓ Successfully saved to: {filepath}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error scraping machines: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec6627",
   "metadata": {},
   "source": [
    "### 6.3 Scrape Reservations\n",
    "\n",
    "This will create **three separate files** divided by type (Machine, Training, Event)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Scraping Reservations\".center(60))\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    reservations_data = client.fetch_all_as_list(\n",
    "        endpoint='/open_api/v1/reservations',\n",
    "        data_key='reservations',\n",
    "        per_page=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Total reservations fetched: {len(reservations_data)}\")\n",
    "    \n",
    "    if reservations_data:\n",
    "        print(f\"\\nSample reservation data:\")\n",
    "        print(json.dumps(reservations_data[0], indent=2, ensure_ascii=False)[:500] + \"...\")\n",
    "    \n",
    "    print(\"\\nDividing reservations by type and saving...\")\n",
    "    saved_files = divide_reservations_by_type(reservations_data, EXPORT_DIR)\n",
    "    \n",
    "    print(f\"\\n✓ Reservations divided and saved by type:\")\n",
    "    for rtype, filepath in saved_files.items():\n",
    "        print(f\"  - {rtype}: {filepath}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error scraping reservations: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab533174",
   "metadata": {},
   "source": [
    "### 6.4 Scrape Trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce24c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Scraping Trainings\".center(60))\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    trainings_data = client.fetch_all_as_list(\n",
    "        endpoint='/open_api/v1/trainings',\n",
    "        data_key='trainings',\n",
    "        per_page=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Total trainings fetched: {len(trainings_data)}\")\n",
    "    \n",
    "    if trainings_data:\n",
    "        print(f\"\\nSample training data:\")\n",
    "        print(json.dumps(trainings_data[0], indent=2, ensure_ascii=False)[:500] + \"...\")\n",
    "    \n",
    "    filepath = save_data(trainings_data, 'trainings', 'Trainings', EXPORT_DIR)\n",
    "    print(f\"\\n✓ Successfully saved to: {filepath}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error scraping trainings: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3085ce4",
   "metadata": {},
   "source": [
    "## 7. Scrape All Data Types at Once\n",
    "\n",
    "Run this cell to scrape all data types in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Scraping ALL Data Types\".center(60))\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "data_types = [\n",
    "    {'name': 'Users', 'endpoint': '/open_api/v1/users', 'data_key': 'users', 'prefix': 'Users'},\n",
    "    {'name': 'Machines', 'endpoint': '/open_api/v1/machines', 'data_key': 'machines', 'prefix': 'Machines'},\n",
    "    {'name': 'Reservations', 'endpoint': '/open_api/v1/reservations', 'data_key': 'reservations', 'prefix': 'Reservations'},\n",
    "    {'name': 'Trainings', 'endpoint': '/open_api/v1/trainings', 'data_key': 'trainings', 'prefix': 'Trainings'}\n",
    "]\n",
    "\n",
    "for data_type in data_types:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping {data_type['name']}...\".center(60))\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    try:\n",
    "        data = client.fetch_all_as_list(\n",
    "            endpoint=data_type['endpoint'],\n",
    "            data_key=data_type['data_key'],\n",
    "            per_page=100\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✓ Total {data_type['name'].lower()} fetched: {len(data)}\")\n",
    "        \n",
    "        if data:\n",
    "            print(f\"\\nSample data:\")\n",
    "            print(json.dumps(data[0], indent=2, ensure_ascii=False)[:300] + \"...\")\n",
    "        \n",
    "        # Special handling for reservations\n",
    "        if data_type['data_key'] == 'reservations' and data:\n",
    "            print(\"\\nDividing reservations by type and saving...\")\n",
    "            saved_files = divide_reservations_by_type(data, EXPORT_DIR)\n",
    "            print(f\"\\n✓ Reservations divided and saved by type:\")\n",
    "            for rtype, filepath in saved_files.items():\n",
    "                print(f\"  - {rtype}\")\n",
    "        else:\n",
    "            filepath = save_data(data, data_type['data_key'], data_type['prefix'], EXPORT_DIR)\n",
    "            print(f\"\\n✓ Successfully saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error scraping {data_type['name']}: {e}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All data types scraped successfully!\".center(60))\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAll files saved to: {EXPORT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc585c48",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Run this cell to see a summary of exported files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca5ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exported Files Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Export directory: {EXPORT_DIR}\\n\")\n",
    "\n",
    "if EXPORT_DIR.exists():\n",
    "    files = sorted(EXPORT_DIR.glob('*.json'), key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    if files:\n",
    "        print(f\"Total files: {len(files)}\\n\")\n",
    "        for i, file in enumerate(files[:20], 1):  # Show last 20 files\n",
    "            size = file.stat().st_size / 1024  # Size in KB\n",
    "            modified = datetime.fromtimestamp(file.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f\"{i:2d}. {file.name}\")\n",
    "            print(f\"    Size: {size:.2f} KB | Modified: {modified}\")\n",
    "        \n",
    "        if len(files) > 20:\n",
    "            print(f\"\\n... and {len(files) - 20} more files\")\n",
    "    else:\n",
    "        print(\"No exported files found yet.\")\n",
    "else:\n",
    "    print(\"Export directory doesn't exist yet.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
